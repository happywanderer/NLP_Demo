{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import standard things\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#import file/os things\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import codecs\n",
    "import pickle\n",
    "\n",
    "#import data science things\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "#Setup data directory variable for global use\n",
    "data_dir = Path.home() / \"Desktop\" / \"bah-intermediate\" / \"CAPSTONE\" / \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download nltk things\n",
    "#Only use if you've not already downloaded nltk things\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get text from files and store in a string variable 'text'\n",
    "def get_text(file_path):\n",
    "    with codecs.open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        text = file.read()\n",
    "            \n",
    "    return text        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process the dataframe, takes 2 arguments: dataframe and language; language defaults to english if none passed\n",
    "def process_df(df, language='english'):\n",
    "    #set stop words based on input language\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    \n",
    "    #lambda below uses list comprehension to create new column in dataframe called 'tokens'; \n",
    "    #the function converts the value of df.text to string then converts to lowercase;\n",
    "    df['tokens'] = df.text.apply(lambda x: [w for w in word_tokenize(str(x).lower())])\n",
    "    \n",
    "    #lambda below uses list comprehension to stem all words in the 'tokens' column\n",
    "    #if the word is not in stop_words and if the word is a word\n",
    "    df['stemmed_list'] = df.tokens.apply(lambda x: [PorterStemmer().stem(w) for w in x \n",
    "                                                          if w not in stop_words and w.isalpha() and len(w) > 2])\n",
    "    \n",
    "    #lambda below uses .join() to create one string from list of words in 'stemmed_list'\n",
    "    df['stemmed'] = df.stemmed_list.apply(lambda x: ' '.join(x))\n",
    "    \n",
    "    # WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. \n",
    "    # By default it is set to Noun\n",
    "    tag_map = defaultdict(lambda : wn.NOUN)\n",
    "    tag_map['J'] = wn.ADJ\n",
    "    tag_map['V'] = wn.VERB\n",
    "    tag_map['R'] = wn.ADV\n",
    "    \n",
    "    # Declaring Empty List to store the words that follow the rules for this step\n",
    "    final_words = []\n",
    "    \n",
    "    for i, entry in enumerate(df.tokens):\n",
    "        #temp_words\n",
    "        temp_words = []\n",
    "        \n",
    "        # Initializing WordNetLemmatizer()\n",
    "        word_lemmatized = WordNetLemmatizer()\n",
    "    \n",
    "        # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "        for word, tag in pos_tag(entry):\n",
    "            # Below condition is to check for Stop words and consider only alphabet\n",
    "            if word not in stop_words and word.isalpha() and len(word) > 2:\n",
    "                word_final = word_lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "                temp_words.append(word_final)\n",
    "        \n",
    "        final_words.append(temp_words)\n",
    "        \n",
    "    # The final processed set of words for each iteration will be stored in 'lemmed_list'\n",
    "    df['lemmed_list'] = final_words\n",
    "    \n",
    "    #lambda below uses .join() to create one string from list of words in 'lemmed_list'\n",
    "    df['lemmed'] = df.lemmed_list.apply(lambda x: ' '.join(x))\n",
    "        \n",
    "    #create a column 'word_count' that is simple count of all words in 'stemmed_list' after processing\n",
    "    df['word_count'] = df.stemmed_list.apply(lambda x: len(x))\n",
    "    \n",
    "    pickle.dump(df, open(data_dir / 'bbc_df_processed.pickle', 'wb'))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declare empty dataframe\n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in files and recursively build dataframe\n",
    "#glob is building iterator for all '.txt' files in directory tree (recursive)\n",
    "#df.append is appending 'topic', 'title', and 'text' for each file\n",
    "for file_path in glob.iglob('./data/**/*.txt', recursive=True):\n",
    "    df = df.append({'topic': file_path.split('\\\\')[1], \n",
    "                    'title': get_text(file_path).split('\\n')[0],\n",
    "                    'text': get_text(file_path)},\n",
    "                    ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(df, open(data_dir / \"bbc_text.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = process_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>topic</th>\n",
       "      <th>tokens</th>\n",
       "      <th>stemmed_list</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmed_list</th>\n",
       "      <th>lemmed</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly...</td>\n",
       "      <td>Ad sales boost Time Warner profit</td>\n",
       "      <td>business</td>\n",
       "      <td>[ad, sales, boost, time, warner, profit, quart...</td>\n",
       "      <td>[sale, boost, time, warner, profit, quarterli,...</td>\n",
       "      <td>sale boost time warner profit quarterli profit...</td>\n",
       "      <td>[sale, boost, time, warner, profit, quarterly,...</td>\n",
       "      <td>sale boost time warner profit quarterly profit...</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar...</td>\n",
       "      <td>Dollar gains on Greenspan speech</td>\n",
       "      <td>business</td>\n",
       "      <td>[dollar, gains, on, greenspan, speech, the, do...</td>\n",
       "      <td>[dollar, gain, greenspan, speech, dollar, hit,...</td>\n",
       "      <td>dollar gain greenspan speech dollar hit highes...</td>\n",
       "      <td>[dollar, gain, greenspan, speech, dollar, hit,...</td>\n",
       "      <td>dollar gain greenspan speech dollar hit high l...</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yukos unit buyer faces loan claim\\n\\nThe owner...</td>\n",
       "      <td>Yukos unit buyer faces loan claim</td>\n",
       "      <td>business</td>\n",
       "      <td>[yukos, unit, buyer, faces, loan, claim, the, ...</td>\n",
       "      <td>[yuko, unit, buyer, face, loan, claim, owner, ...</td>\n",
       "      <td>yuko unit buyer face loan claim owner embattl ...</td>\n",
       "      <td>[yukos, unit, buyer, face, loan, claim, owner,...</td>\n",
       "      <td>yukos unit buyer face loan claim owner embattl...</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>High fuel prices hit BA's profits\\n\\nBritish A...</td>\n",
       "      <td>High fuel prices hit BA's profits</td>\n",
       "      <td>business</td>\n",
       "      <td>[high, fuel, prices, hit, ba, 's, profits, bri...</td>\n",
       "      <td>[high, fuel, price, hit, profit, british, airw...</td>\n",
       "      <td>high fuel price hit profit british airway blam...</td>\n",
       "      <td>[high, fuel, price, hit, profit, british, airw...</td>\n",
       "      <td>high fuel price hit profit british airway blam...</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pernod takeover talk lifts Domecq\\n\\nShares in...</td>\n",
       "      <td>Pernod takeover talk lifts Domecq</td>\n",
       "      <td>business</td>\n",
       "      <td>[pernod, takeover, talk, lifts, domecq, shares...</td>\n",
       "      <td>[pernod, takeov, talk, lift, domecq, share, dr...</td>\n",
       "      <td>pernod takeov talk lift domecq share drink foo...</td>\n",
       "      <td>[pernod, takeover, talk, lift, domecq, share, ...</td>\n",
       "      <td>pernod takeover talk lift domecq share drink f...</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Ad sales boost Time Warner profit\\n\\nQuarterly...   \n",
       "1  Dollar gains on Greenspan speech\\n\\nThe dollar...   \n",
       "2  Yukos unit buyer faces loan claim\\n\\nThe owner...   \n",
       "3  High fuel prices hit BA's profits\\n\\nBritish A...   \n",
       "4  Pernod takeover talk lifts Domecq\\n\\nShares in...   \n",
       "\n",
       "                               title     topic  \\\n",
       "0  Ad sales boost Time Warner profit  business   \n",
       "1   Dollar gains on Greenspan speech  business   \n",
       "2  Yukos unit buyer faces loan claim  business   \n",
       "3  High fuel prices hit BA's profits  business   \n",
       "4  Pernod takeover talk lifts Domecq  business   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [ad, sales, boost, time, warner, profit, quart...   \n",
       "1  [dollar, gains, on, greenspan, speech, the, do...   \n",
       "2  [yukos, unit, buyer, faces, loan, claim, the, ...   \n",
       "3  [high, fuel, prices, hit, ba, 's, profits, bri...   \n",
       "4  [pernod, takeover, talk, lifts, domecq, shares...   \n",
       "\n",
       "                                        stemmed_list  \\\n",
       "0  [sale, boost, time, warner, profit, quarterli,...   \n",
       "1  [dollar, gain, greenspan, speech, dollar, hit,...   \n",
       "2  [yuko, unit, buyer, face, loan, claim, owner, ...   \n",
       "3  [high, fuel, price, hit, profit, british, airw...   \n",
       "4  [pernod, takeov, talk, lift, domecq, share, dr...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  sale boost time warner profit quarterli profit...   \n",
       "1  dollar gain greenspan speech dollar hit highes...   \n",
       "2  yuko unit buyer face loan claim owner embattl ...   \n",
       "3  high fuel price hit profit british airway blam...   \n",
       "4  pernod takeov talk lift domecq share drink foo...   \n",
       "\n",
       "                                         lemmed_list  \\\n",
       "0  [sale, boost, time, warner, profit, quarterly,...   \n",
       "1  [dollar, gain, greenspan, speech, dollar, hit,...   \n",
       "2  [yukos, unit, buyer, face, loan, claim, owner,...   \n",
       "3  [high, fuel, price, hit, profit, british, airw...   \n",
       "4  [pernod, takeover, talk, lift, domecq, share, ...   \n",
       "\n",
       "                                              lemmed  word_count  \n",
       "0  sale boost time warner profit quarterly profit...         221  \n",
       "1  dollar gain greenspan speech dollar hit high l...         212  \n",
       "2  yukos unit buyer face loan claim owner embattl...         149  \n",
       "3  high fuel price hit profit british airway blam...         216  \n",
       "4  pernod takeover talk lift domecq share drink f...         152  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(processed_df.word_count, bins =\n",
    "len(set(processed_df.word_count)))\n",
    "plt.xlabel('Number of words per Article')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.word_count.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.topic.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.query('word_count > 500').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.query('word_count > 500').topic.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2200.000000\n",
       "mean      198.202273\n",
       "std        86.017350\n",
       "min        46.000000\n",
       "25%       132.000000\n",
       "50%       178.000000\n",
       "75%       251.000000\n",
       "max       499.000000\n",
       "Name: word_count, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimmed_df = processed_df.query('word_count <= 500')\n",
    "trimmed_df.word_count.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(trimmed_df.word_count, bins =\n",
    "len(set(trimmed_df.word_count)))\n",
    "plt.xlabel('Number of words per Article')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(trimmed_df, open(data_dir / 'bbc_df_trimmed.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
